monitor-paypal
==============

The `monitor-paypal` module provides a middleware for monitoring and reporting key application metrics to allow ops/devops/tools to see how an application is performing in LIVE and when it is in trouble. Some of the metrics reported include:
 - event loop time
 - Number of full garbage collections and frequeny of such GCs
 - Used heap memory recovery level after a full GC
 - Suspected memory leak reporting
 - CPU usage
 - Request/second and number of concurrent requests handled

# Installation

```
npm install monitor-paypal --save
```

# Usage

## Middleware
In order to use the 'monitor-paypal'  module, the following middleware must be enabled:

```javascript
app.use(require('monitor-paypal').init([options]) );
```

Note that in a Kraken 1.0+ app derived from sample-app, this will be automatically enabled by the infrastructure and options would be configured in config files as below, however there
is likely nothing you need to configure.

**config.json**
```
{
    "middleware": {
        "monitor-paypal": {
          "enabled": true,
          "priority": 118,
          "module": {
            "name": "monitor-paypal",
            "method": "init"
          }
        }
    },

    ...

    "monitor": {
        "toAddr": "<DL addr>"
    }
}
```

If you have no interest in monitoring request-related metrics, you can just enable the other metrics by doing:

```javascript
require('monitor-paypal').init([options]);
```

After the above middleware runs, monitoring will be enabled to run periodically and report metrics to Sherlock.
In addition to Sherlock reporting, statistics are emitted as an event you can listen to
and use however you want. To do that:

```javascript
var Monitor = require('monitor-paypal');
Monitor.on('monitorStats', function(stats) {
    ... do something with stats
});
```

# API

## Configuration

Configuration can be defined by an options param or via a config file.
Configuration options are:
* **pool:** name of the pool (determined automatically but you can override)
* **clientEndPoint:** The end point for receiving Sherlock metrics reporting. It is set automatically
  for production and pre-production. It is defaulted to empty for development to avoid cluttering
  the log with metrics chatter once a minute. For other environments, it goes to the non-production
  end point. You normally will not set this unless you have special needs. .
* **maxLag:** The number of event loop milliseconds that constitutes "too busy" and results in a call
  to the tooBusyHandler.
* **tooBusyHandler:** If maxLag is exceeded, the applications "tooBusyHandler" will be given control
  to deal with the situation. A common response is to return a 503 response page.
  If you don't want any tooBusy handling, just don't specify a handler.
  If a value is specified, a `require(tooBusyHandler)` will be done to obtain the function to call.
  The function will be called with parameters (request, response, next) just like a route receives.
* **maxOpenRequests:** maximum number of received but not completed requests. If the number
  in process exceeds this amount, an HTTP 503 response will be returned. This is a
  way to prevent unbounded demands on the app when it may be blocked by services that
  are down. If back pressure is not exerted, new requests continue to be accepted and
  added to the event loop using more and more memory. Ideally, the app itself will
  deal with down or timing out services in a more graceful way so this is just
  a backup protection. Set it to a number you never expect your app to support in
  one processor at one time. Default is 0 which means no limit.

Sample configuration options:

```
{
    "tooBusyHandler": "path:./lib/tooBusyHandler"
}
```

`maxLag` would typically not be configured but is shown here. This is the
amount of time to go around the NodeJs event loop. If it exceeds the default 70 msec.
your app is probably in trouble and needs to exert backpressure on the user.
If you need some other value, you can configure it (but not lower than 10).
If configured, the tooBusyHandler will be called to generate a response to the user.

`clientGroup` is used to connect similar logged data from multiple instances
of the application and to support authorization to send metrics and quota.
This is determined for your application.

The `clientEndPoint` will be provided by the infrastructure but if you need
to override it, you can.

If the application heap usage exceeds 1.2GB, the application will be killed
and there is a presumption that forever, pm2 or some other app manager will
restart it. If the environment variable `NODE_HEAPDUMP_OPTIONS=dump` is set,
a full heapdump will be written to the logs directory. It is not written
by default out of concern for putting PII data on disk.

If a restart of the application is detected, an email will
be sent to a monitoring DL plus any other DL configured in config/config.json.
The mails are sent in production. If you add a `stageToAddr`
to the config, you can force mails to be sent for crashes on stage machines.
Note that stageToAddr overrides whatever is in monitor:toAddr. The
`toAddr` and `stageToAddr` values can be a comma-separated list of
email addresses.

```
"monitor": {
    "toAddr": "whoYouAre@paypal.com"
    "stageToAddr": "xxxx@paypal.com"
}
```

In addition to the crash email, every crash restart will be logged to CAL
(crash emails only send the first 25 times). The CAL event type is CRASH and
the event should have the relevant part of the error log.

Note: you should not use a @paypalcorp.com mail domain. Your mail will not arrive.

# Additional Information

## Reported metrics

Memory related

- `gc_count` number of full garbage collects
- `gc_count_incremental` number of incremental garbage collects
- `heapSizePostGC`
- `gcInterval` percent of total CPU spent doing GC
- `rss` (memory used for compiled js/jit code)
- `heapUsed`
- `heapTotal`
- `memoryLeakReports` (count of instances of Out-of-Memory restarts)

CPU/session/app related

- `cpu` (% CPU used)
- `uptime`
- `eventLoop` (msec to go around event loop)
- `availableFileDescriptors`
- `noWorkers`
- `restarts` (# of times your app has restarted)

Request Processing related

- `urlTime`
- `renderTime`
- `concurrentRequests`
- `cookieSizeMax`
- `requestBodySize`
- `sessionSize`
- `tps` (transactions/second aka requests/second)
- `eps` (errors/second)
- `http2xx` (Number of 200 responses)
- `http3xx` (Number of 300 responses)
- `http4xx` (Number of 400 responses)
- `http5xx` (Number of 500 responses)

## TSDB monitoring
A basic tool for viewing your monitoring data, TSDB, is available for
the various endpoints your app might report to.

The URL for exploring LIVE metrics is:
http://sherlock-qs-vip-b.slc.paypalinc.com/

The QA URL for exploring QA-generated metrics is:
http://sherlock-qs-vip.qa.paypal.com

The TSDB we run is based on an open-source product. The documentation
will help you to use the tool but our version is a bit older than
what is documented so minor differences may exist.

http://opentsdb.net/docs/build/html/index.html

## Custom Dashboard
Sherlock provides a mechanism for you to define a custom dashboard and see
graphs of various metrics: https://sherlock-vip.paypalinc.com/MySherlockApp/MyDashboards

You can click the big blue "Create Dashboard" button to get started.

The next screen provides a series of settings you need to work throgh:
* First choose Marketplaces or PayPal depending on where your app runs
* Then click on the "Profile" drop-down and start typing nodejs_perfmon. Before you get far, it will be the only choice so click the radio button to select it.
* Then click on the "Metric" drop-down and select one of the above metrics -- just start typing the name until it is the only one visible and then check the radio button. e.g., type "cpu" and you can check nodejs_perfmon.cpu
* Then click on the "Dimensions" drop-down and select the granularity of what you want to graph. "pool" is likely to be the most common but some may which to graph by colo or host.
* If you choose pool, then the "Dimension Value(s) drop-down should show your pool name (assuming your app has been emitting data already). You cannot preconfigure a dashboard until your app has begun sending data. Check the "Dimension Value(s) you are interested in.
* If you click the "Add  Graph To" button and choose "Default Graph", the single set of dimension data will be added to the dashboard as a graph. If you want to plot more than one dimension of data on the same graph, click the big "+" at the right end  of the dimension defintion dropdowns line, and you can define more dimensions. When you have all you want,use "Add Graph To" in order to create the graph.
* When you are done defining graphs, use the "Save Dashboard" at the bottom right to save all your work and create the  Custom Dashboard. A popup dialogs lets you name the dashboard and control the degree of sharing. There are other things you can do but I defer to the Sherlock team documentation for those.

## Metrics Details

For those wanting more insight into exactly how each metric is computed

```
 name: 'eventLoop',       // node event loop time 'GAUGE', Integer, units: 'msec.'
 name: 'gc_count',        // # of full garbace collects in this interval 'GAUGE', Integer
 name: 'gc_count_incremental', // # of incremental garbage collects in this interval 'GAUGE', Integer
 name: 'heapSizePostGC'   // Heap size after a full GC 'GAUGE', Integer, units: 'Kbytes'
 name: 'heap_max'         // Maximum heap size. Note node periodically bumps this up'GAUGE', Integer, units: 'Kbytes'
 name: 'heap_used'        // amount of heap currently used 'GAUGE', Integer, units: 'Kbytes'
 name: 'rss'              // V8's memory usage 'GAUGE', Integer, units: 'Kbytes'
 name: 'gcInterval'       // % of cp time spent doing GC in the interval 'GAUGE', Float
 name: 'memoryLeakReports'  // Number of out-of-memory events since last deploy 'COUNTER', Integer
 name: 'uptime'           // node's process.uptime value 'COUNTER', Integer, units: 'sec'
 name: 'concurrentRequests' // Maximum # of requests in process at once in last 60 seconds 'GAUGE', Integer
 name: 'urlTime'          // median url time in last 60 seconds 'GAUGE', Float, units: 'msec'
 name: 'fd_available'     // # of available file descriptors at end of each 60 second reporting interval 'GAUGE', Integer
 name: 'renderTime'       // Median render time in last 60 seconds 'GAUGE', Float, units: 'msec'
 name: 'eps'              // # 4xx/5xx responses/60 seconds 'GAUGE', Float
 name: '2XXCount'         // Count of 5xx responses in last 60 seconds 'COUNTER', Integer
 name: '3XXCount'         // Count of 5xx responses in last 60 seconds 'COUNTER', Integer
 name: '4XXCount'         // Count of 4xx responses in last 60 seconds 'COUNTER', Integer
 name: '5XXCount'         // Count of 5xx responses in last 60 seconds 'COUNTER', Integer
 name: 'RequestBodySize'  // Median size seen in last 60 seconds 'GAUGE', Float, units: 'bytes'
 name: 'MaxCookieSize'    // Max size seen each 60 seconds 'GAUGE', Integer, units: 'bytes'
 name: 'sessionSize'      // Median session size 'GAUGE', Integer, units: 'bytes'
 name: 'tps'              // Transactions/second # transactions/60 seconds 'GAUGE', Float
 name: 'cpu'              // Percent CPU usage 'GAUGE', Float, units: 'percentage'
 name: 'noWorkers'        // Number of node instances running 'GAUGE', Integer
 name: 'restarts'         // Number of restarts since last deploy 'GAUGE', Integer
```

## Debugging using monitor-paypal

In times of crisis or when node process in trouble, you can get some additional debugging and dumps with the help of this module.

 - **memory watch for leaks**
    - Runs the memwatch for 2min (120000ms)
    - writes the memory leak info on `logs/err.log` file
    - more details - https://github.com/marcominetti/node-memwatch
    - urls
      - Locally: http://localhost:8000/[requestUri]/meta_debug?operation=memwatch&timeout=120000
      - Stage/Live: https://localhost/[requestUri]/meta_debug?operation=memwatch&timeout=120000
 - **js heapdump**
    - Runs the heapdump for 1min (60000ms)
    - dumps the heapdump file under `logs` folder.
    - This dump file can be analyzed using chrome profiler.
    - more details - https://github.com/bnoordhuis/node-heapdump
    - urls
      - Locally: http://localhost:8000/[requestUri]/meta_debug?operation=heapdump
      - Stage/Live: https://localhost/[requestUri]/meta_debug?operation=heapdump
 - **js heapdiff**
    - Runs the heapdiff for 1min (60000ms)
    - dumps a heapdiff JSON file under `logs` folder.
    - more details - https://github.com/marcominetti/node-memwatch
    - urls
      - Locally: http://localhost:8000/[requestUri]/meta_debug?operation=heapdiff&timeout=60000
      - Stage/Live: https://localhost/[requestUri]/meta_debug?operation=heapdiff&timeout=60000
 - **v8profiler**
    - Runs the v8profiler for 1min (60000ms)
    - Will restart the process after it's run (should be used with caution)
    - dumps the v8 profiler output file under `logs` folder.
    - more details - https://github.com/node-inspector/v8-profiler
    - urls
      - Locally: http://localhost:8000/[requestUri]/meta_debug?operation=v8profiler&timeout=60000
      - Stage/Live: https://localhost/[requestUri]/meta_debug?operation=v8profiler&timeout=60000

**Notes**
 - This debugging required you to be accessing the url on `localhost`.
 - You need to curl the above url in stage or LIVE to start this debugging.
 - This is a timed debugging operation. Specify a different timeout if you want to run it longer.
 - Delete the debugging output files from the machines manually. As running these operations too many times and for longer durations can be costly on disk space.
